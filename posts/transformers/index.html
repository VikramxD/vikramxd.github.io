<!DOCTYPE html>
<html lang="en" dir="auto">
<head>
    
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Implementing the Transformer Model from Scratch </title>


<meta name="keywords" content="ML, Engineering, Systems">



<meta name="description" content="A super comprehensive and detailed guide to understanding the Transformer model and implementing it using PyTorch.">





<link rel="canonical" href="https:&#x2F;&#x2F;vikramxd.github.io&#x2F;posts&#x2F;transformers&#x2F;">


<link rel="stylesheet" href="https://vikramxd.github.io/css/includes/scroll-bar.css">
<link rel="stylesheet" href="https://vikramxd.github.io/css/styles.css">
<link rel="stylesheet" href="https://vikramxd.github.io/css/override.css">







<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }
    </style>
    
</noscript>



    
</head>
<body class=" dark" id="top">
    
    
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }
</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https:&#x2F;&#x2F;vikramxd.github.io" accesskey="h" title="Applied Mode (Alt + H)">
                Applied Mode
            </a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch">
                    <li></li>
                </ul>
            </div>
        </div>
        
        <ul id="menu">
            
            
            
            <li>
                <a href="https:&#x2F;&#x2F;vikramxd.github.io&#x2F;archive" title="Archive">
                    <span>Archive</span>
                    
                </a>
            </li>
            
            
            
            <li>
                <a href="https:&#x2F;&#x2F;vikramxd.github.io&#x2F;tags" title="Tags">
                    <span>Tags</span>
                    
                </a>
            </li>
            
            
            
            <li>
                <a href="https:&#x2F;&#x2F;vikramxd.github.io&#x2F;search" title="Search">
                    <span>Search</span>
                    
                </a>
            </li>
            
            
            
            <li>
                <a href="https:&#x2F;&#x2F;docs.google.com&#x2F;document&#x2F;d&#x2F;1A8KGNkAtYyi4zIxSFWQ_mwBqhbXn8Pu_qbSNzDHwaws&#x2F;edit?usp=sharing" title="Resume">
                    <span>Resume</span>
                    
                </a>
            </li>
            
        </ul>
        
    </nav>
</header>

    
    <main class="main">
        
<article class="post-single">
  <header class="post-header">
      <div class="breadcrumbs">
          <a href="https:&#x2F;&#x2F;vikramxd.github.io">Home</a>&nbsp;¬ª&nbsp;
          <a href="https://vikramxd.github.io/posts/">Posts</a>&nbsp;¬ª&nbsp;
          <a href="https:&#x2F;&#x2F;vikramxd.github.io&#x2F;posts&#x2F;transformers&#x2F;">Implementing the Transformer Model from Scratch </a>
      </div>
      <h1 class="post-title">Implementing the Transformer Model from Scratch </h1>
      
      <div class="post-description">
          A super comprehensive and detailed guide to understanding the Transformer model and implementing it using PyTorch.
      </div>
      
      
      <div class="post-meta">
      














<span title="2024-10-23 00:00:00 +0000">23 October 2024</span>&nbsp;¬∑&nbsp;11 min&nbsp;¬∑&nbsp;2140 words&nbsp;¬∑&nbsp;

      
      </div>
      
  </header>

  
  

<div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner">
            <ul>
                
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a>
                    
                </li>
                
                <li>
                    <a href="#main-components-of-the-transformer-architecture" aria-label="Main Components of the Transformer Architecture">Main Components of the Transformer Architecture</a>
                    
                </li>
                
                <li>
                    <a href="#input-embeddings" aria-label="Input Embeddings">Input Embeddings</a>
                    
                    <ul>
                    
                        <li>
                            <a href="#implementation-of-the-input-embedding-block" aria-label="Implementation of the Input Embedding Block">Implementation of the Input Embedding Block</a>
                            
                        </li>
                    
                    </ul>
                    
                </li>
                
                <li>
                    <a href="#positional-encoding" aria-label="Positional Encoding">Positional Encoding</a>
                    
                    <ul>
                    
                        <li>
                            <a href="#implementation-of-the-positional-encoding-block" aria-label="Implementation of the Positional Encoding Block">Implementation of the Positional Encoding Block</a>
                            
                        </li>
                    
                    </ul>
                    
                </li>
                
                <li>
                    <a href="#layer-normalization" aria-label="Layer Normalization">Layer Normalization</a>
                    
                </li>
                
                <li>
                    <a href="#simple-mlp-or-feedforward-block" aria-label="Simple MLP or FeedForward Block">Simple MLP or FeedForward Block</a>
                    
                </li>
                
                <li>
                    <a href="#implemention-of-the-feedforward-block" aria-label="Implemention of the FeedForward Block">Implemention of the FeedForward Block</a>
                    
                </li>
                
                <li>
                    <a href="#multihead-attention-block" aria-label="Multihead Attention Block">Multihead Attention Block</a>
                    
                </li>
                
                <li>
                    <a href="#implementation-of-multihead-attention-block" aria-label="Implementation of Multihead Attention Block">Implementation of Multihead Attention Block</a>
                    
                </li>
                
                <li>
                    <a href="#encoder-block" aria-label="Encoder Block">Encoder Block</a>
                    
                </li>
                
                <li>
                    <a href="#implementation-of-the-encoder-block" aria-label="Implementation of the Encoder Block">Implementation of the Encoder Block</a>
                    
                </li>
                
                <li>
                    <a href="#decoder-block" aria-label="Decoder Block">Decoder Block</a>
                    
                </li>
                
                <li>
                    <a href="#implementation-of-the-decoder-block" aria-label="Implementation of the Decoder Block">Implementation of the Decoder Block</a>
                    
                </li>
                
                <li>
                    <a href="#building-the-transformer" aria-label="Building the Transformer">Building the Transformer</a>
                    
                </li>
                
            </ul>
        </div>
    </details>
</div>


  

  
  <div class="post-content">
      <h3 id="introduction">Introduction<a class="anchor" aria-hidden="true" href="#introduction" hidden="">#</a>
</h3>
<p>The Transformer, with its parallel processing capabilities, allowed for more efficient and scalable models, making it easier to train them on large datasets. It also demonstrated superior performance in several NLP tasks, such as sentiment analysis and text generation tasks. The Transformer model has since become the foundation for many state-of-the-art NLP models, such as BERT, GPT-2, and T5.</p>
<h3 id="main-components-of-the-transformer-architecture">Main Components of the Transformer Architecture<a class="anchor" aria-hidden="true" href="#main-components-of-the-transformer-architecture" hidden="">#</a>
</h3>
<p><img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers-dark.svg" alt="Transformer Architecture" /></p>
<p>The Transformer Architecture Consists of 2 main Components -&gt;</p>
<ul>
<li>
<p>Encoder -  The encoder receives an input and builds an Embedding of it's Features. This means that the model learns to understand the association of words or sequence.</p>
</li>
<li>
<p>Decoder - The decoder uses the encoder‚Äôs output embeddings along with other inputs to generate a target sequence.</p>
</li>
</ul>
<h3 id="input-embeddings">Input Embeddings<a class="anchor" aria-hidden="true" href="#input-embeddings" hidden="">#</a>
</h3>
<p>Whenever we use a dataset , and try to Train a Model on it , we always convert explicitly or implicitly to a representation which the model can interpret / understand and then reconvert it into a representation we understand , the Function of Input Embedding Block in the Transformer Architecture is just that only. In the Orignal Paper the Authors used , the Input Block with an Embedding Dimension of 512.To prevent the input Embeddings from being extremely small , we normalize them by Multiplying the by root of EmbeddingDimension</p>
<h4 id="implementation-of-the-input-embedding-block">Implementation of the Input Embedding Block<a class="anchor" aria-hidden="true" href="#implementation-of-the-input-embedding-block" hidden="">#</a>
</h4>
<pre data-lang="python" style="background-color:#383838;color:#e6e1dc;" class="language-python "><code class="language-python" data-lang="python"><span>    </span><span style="color:#f92672dd;">import </span><span>torch
</span><span>    </span><span style="color:#f92672dd;">import </span><span>torch.nn </span><span style="color:#cc7833;">as </span><span>nn
</span><span>    </span><span style="color:#f92672dd;">import </span><span>math
</span><span>        
</span><span>    </span><span style="font-style:italic;color:#cc7833;">class </span><span style="text-decoration:underline;color:#ffc66d;">InputEmbeddingBlock</span><span>(</span><span style="text-decoration:underline;font-style:italic;color:#ffc66d;">nn.Module</span><span>):
</span><span>
</span><span>        </span><span style="font-style:italic;color:#cc7833;">def </span><span style="color:#da4939;">__init__</span><span>(</span><span style="font-style:italic;color:#fd971f;">self</span><span>,</span><span style="font-style:italic;color:#fd971f;">embedding_dim</span><span>:</span><span style="font-style:italic;color:#6e9cbe;">int</span><span>,</span><span style="font-style:italic;color:#fd971f;">vocab_size</span><span>:</span><span style="font-style:italic;color:#6e9cbe;">int</span><span>):
</span><span>                
</span><span>            </span><span style="color:#da4939;">super</span><span>().</span><span style="color:#da4939;">__init__</span><span>()
</span><span>            </span><span style="color:#d0d0ff;">self</span><span>.embedding_dim </span><span style="color:#cc7833;">= </span><span>embedding_dim </span><span style="color:#95815e;"># Reffered in the paper as d_model, (Size == 512)
</span><span>            </span><span style="color:#d0d0ff;">self</span><span>.vocab_size </span><span style="color:#cc7833;">= </span><span>vocab_size </span><span style="color:#95815e;">## Size of the Vocabulary of the input 
</span><span>            </span><span style="color:#d0d0ff;">self</span><span>.embedding </span><span style="color:#cc7833;">= </span><span>nn.Embedding(</span><span style="color:#d0d0ff;">self</span><span>.vocab_size,</span><span style="color:#d0d0ff;">self</span><span>.embedding_dim)
</span><span>
</span><span>        </span><span style="font-style:italic;color:#cc7833;">def </span><span style="color:#ffc66d;">forward</span><span>(</span><span style="font-style:italic;color:#fd971f;">self</span><span>,</span><span style="font-style:italic;color:#fd971f;">x</span><span>):
</span><span>            </span><span style="color:#cc7833;">return </span><span style="color:#d0d0ff;">self</span><span>.embedding_dim(x)</span><span style="color:#cc7833;">*</span><span>math.sqrt(</span><span style="color:#d0d0ff;">self</span><span>.embedding_dim) </span><span style="color:#95815e;">## This is done to help Prevent the Size of Input Embedding being diminished
</span><span>
</span></code></pre>
<h3 id="positional-encoding">Positional Encoding<a class="anchor" aria-hidden="true" href="#positional-encoding" hidden="">#</a>
</h3>
<p>The input now is converted into input Embeddings of Dimension 512 , but unless we don't provide a signal for the encoder on the relative or absolute position of the tokens in the sequence the Model can't learn the corresponding association
to get around that probelm the authors have provided a positional Encoding for a token based on if its index is an even number or an odd number , these encodings are computed only once and in the paper are not learned by the model.</p>
<h4 id="implementation-of-the-positional-encoding-block">Implementation of the Positional Encoding Block<a class="anchor" aria-hidden="true" href="#implementation-of-the-positional-encoding-block" hidden="">#</a>
</h4>
<pre data-lang="python" style="background-color:#383838;color:#e6e1dc;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#f92672dd;">import </span><span>torch
</span><span style="color:#f92672dd;">import </span><span>torch.nn </span><span style="color:#cc7833;">as </span><span>nn
</span><span>
</span><span style="font-style:italic;color:#cc7833;">class </span><span style="text-decoration:underline;color:#ffc66d;">PositionalEncoding</span><span>(</span><span style="text-decoration:underline;font-style:italic;color:#ffc66d;">nn.Module</span><span>):
</span><span>    </span><span style="color:#95815e;">&quot;&quot;&quot;
</span><span style="color:#95815e;">    Positional Encoding module for Transformer models.
</span><span style="color:#95815e;">
</span><span style="color:#95815e;">    Args:
</span><span style="color:#95815e;">        embedding_dim (int): The dimension of the input embeddings.
</span><span style="color:#95815e;">        sequence_len (int): The length of the input sequence.
</span><span style="color:#95815e;">        dropout (float): The dropout probability.
</span><span style="color:#95815e;">    &quot;&quot;&quot;
</span><span>    </span><span style="font-style:italic;color:#cc7833;">def </span><span style="color:#da4939;">__init__</span><span>(</span><span style="font-style:italic;color:#fd971f;">self</span><span>, </span><span style="font-style:italic;color:#fd971f;">embedding_dim</span><span>: </span><span style="font-style:italic;color:#6e9cbe;">int</span><span>, </span><span style="font-style:italic;color:#fd971f;">sequence_len</span><span>: </span><span style="font-style:italic;color:#6e9cbe;">int</span><span>, </span><span style="font-style:italic;color:#fd971f;">dropout</span><span>: </span><span style="font-style:italic;color:#6e9cbe;">float</span><span>):
</span><span>        </span><span style="color:#da4939;">super</span><span>().</span><span style="color:#da4939;">__init__</span><span>()
</span><span>        </span><span style="color:#d0d0ff;">self</span><span>.embedding_dim </span><span style="color:#cc7833;">= </span><span>embedding_dim
</span><span>        </span><span style="color:#d0d0ff;">self</span><span>.sequence_len </span><span style="color:#cc7833;">= </span><span>sequence_len
</span><span>        </span><span style="color:#d0d0ff;">self</span><span>.dropout </span><span style="color:#cc7833;">= </span><span>nn.Dropout(dropout)
</span><span>
</span><span>        </span><span style="color:#95815e;"># Creating a matrix of size (sequence_len,embedding_dim)
</span><span>        positional_encoding </span><span style="color:#cc7833;">= </span><span>torch.zeros(sequence_len, </span><span style="color:#d0d0ff;">self</span><span>.embedding_dim)
</span><span>
</span><span>        </span><span style="color:#95815e;"># Create a vector of shape (sequence_len,1)
</span><span>        position </span><span style="color:#cc7833;">= </span><span>torch.arange(</span><span style="color:#a5c261;">0</span><span>, sequence_len, </span><span style="font-style:italic;color:#fd971f;">dtype</span><span style="color:#cc7833;">=</span><span>torch.float).unsqueeze(</span><span style="font-style:italic;color:#fd971f;">dim</span><span style="color:#cc7833;">=</span><span style="color:#a5c261;">1</span><span>)
</span><span>        division_term </span><span style="color:#cc7833;">= </span><span>torch.exp(torch.arange(</span><span style="color:#a5c261;">0</span><span>, embedding_dim, </span><span style="color:#a5c261;">2</span><span>)).float() </span><span style="color:#cc7833;">* </span><span>(</span><span style="color:#cc7833;">-</span><span>torch.log(</span><span style="color:#a5c261;">10000.0</span><span>) </span><span style="color:#cc7833;">/ </span><span>embedding_dim)
</span><span>
</span><span>        </span><span style="color:#95815e;"># Apply the sin formula to the even positions and cosine formula to the odd positions
</span><span>        positional_encoding[:, </span><span style="color:#a5c261;">0</span><span>::</span><span style="color:#a5c261;">2</span><span>] </span><span style="color:#cc7833;">= </span><span>torch.sin(position </span><span style="color:#cc7833;">* </span><span>division_term) </span><span style="color:#95815e;"># Every two Terms even -&gt; 0 -&gt; 2 -&gt; 4 
</span><span>        positional_encoding[:, </span><span style="color:#a5c261;">1</span><span>::</span><span style="color:#a5c261;">2</span><span>] </span><span style="color:#cc7833;">= </span><span>torch.cos(position </span><span style="color:#cc7833;">* </span><span>division_term) </span><span style="color:#95815e;"># Every two Terms odd -&gt; 1 -&gt; 3 -&gt; 5
</span><span>        
</span><span>        positional_encoding </span><span style="color:#cc7833;">= </span><span>positional_encoding.unsqueeze(</span><span style="font-style:italic;color:#fd971f;">dim</span><span style="color:#cc7833;">=</span><span style="color:#a5c261;">0</span><span>)
</span><span>        </span><span style="color:#d0d0ff;">self</span><span>.register_buffer(</span><span style="color:#a5c261;">&#39;positional_encoding&#39;</span><span>,positional_encoding)
</span><span>        
</span><span>    </span><span style="font-style:italic;color:#cc7833;">def </span><span style="color:#ffc66d;">forward</span><span>(</span><span style="font-style:italic;color:#fd971f;">self</span><span>,</span><span style="font-style:italic;color:#fd971f;">x</span><span>):
</span><span>        x </span><span style="color:#cc7833;">= </span><span>x </span><span style="color:#cc7833;">+ </span><span>(</span><span style="color:#d0d0ff;">self</span><span>.pe[:,:x.shape[</span><span style="color:#a5c261;">1</span><span>],:]).requires_grad(</span><span style="color:#6e9cbe;">False</span><span>)
</span><span>        </span><span style="color:#cc7833;">return </span><span style="color:#d0d0ff;">self</span><span>.dropout(x)
</span><span>
</span></code></pre>
<h3 id="layer-normalization">Layer Normalization<a class="anchor" aria-hidden="true" href="#layer-normalization" hidden="">#</a>
</h3>
<p>These are the Add and Norm Layer in the Architecture these help scaling input tensor with Layer the LayerNormalization Block is already implemented in Pytorch</p>
<pre data-lang="python" style="background-color:#383838;color:#e6e1dc;" class="language-python "><code class="language-python" data-lang="python"><span>
</span><span style="color:#f92672dd;">import </span><span>torch
</span><span style="color:#f92672dd;">import </span><span>torch.nn </span><span style="color:#cc7833;">as </span><span>nn
</span><span>
</span><span>
</span><span style="font-style:italic;color:#cc7833;">class </span><span style="text-decoration:underline;color:#ffc66d;">LayerNormalization</span><span>(</span><span style="text-decoration:underline;font-style:italic;color:#ffc66d;">nn.Module</span><span>):
</span><span>    </span><span style="color:#95815e;">&quot;&quot;&quot;
</span><span style="color:#95815e;">    Applies layer normalization to the input tensor.
</span><span style="color:#95815e;">
</span><span style="color:#95815e;">    Args:
</span><span style="color:#95815e;">        eps (float, optional): A value added to the denominator for numerical stability. Default is 1e-5.
</span><span style="color:#95815e;">    &quot;&quot;&quot;
</span><span>
</span><span>    </span><span style="font-style:italic;color:#cc7833;">def </span><span style="color:#da4939;">__init__</span><span>(</span><span style="font-style:italic;color:#fd971f;">self</span><span>, </span><span style="font-style:italic;color:#fd971f;">eps</span><span>: </span><span style="font-style:italic;color:#6e9cbe;">float </span><span style="color:#cc7833;">= </span><span style="color:#a5c261;">1e-5</span><span>) -&gt; </span><span style="color:#6e9cbe;">None</span><span>:
</span><span>        </span><span style="color:#d0d0ff;">self</span><span>.eps </span><span style="color:#cc7833;">= </span><span>eps
</span><span>
</span><span>    </span><span style="font-style:italic;color:#cc7833;">def </span><span style="color:#ffc66d;">forward</span><span>(</span><span style="font-style:italic;color:#fd971f;">self</span><span>, </span><span style="font-style:italic;color:#fd971f;">x</span><span>):
</span><span>        </span><span style="color:#95815e;">&quot;&quot;&quot;
</span><span style="color:#95815e;">        Applies layer normalization to the input tensor.
</span><span style="color:#95815e;">
</span><span style="color:#95815e;">        Args:
</span><span style="color:#95815e;">            x (torch.Tensor): The input tensor.
</span><span style="color:#95815e;">
</span><span style="color:#95815e;">        Returns:
</span><span style="color:#95815e;">            torch.Tensor: The normalized tensor.
</span><span style="color:#95815e;">        &quot;&quot;&quot;
</span><span>        </span><span style="color:#cc7833;">return </span><span>nn.LayerNorm(x, </span><span style="font-style:italic;color:#fd971f;">eps</span><span style="color:#cc7833;">=</span><span style="color:#d0d0ff;">self</span><span>.eps)
</span><span>
</span><span>
</span></code></pre>
<h3 id="simple-mlp-or-feedforward-block">Simple MLP or FeedForward Block<a class="anchor" aria-hidden="true" href="#simple-mlp-or-feedforward-block" hidden="">#</a>
</h3>
<p>Really Simple MLP consisting of 2 Linear Layers with the ReLU activation function b/w them also using Dropout Layer for overfitting prevention.</p>
<h3 id="implemention-of-the-feedforward-block">Implemention of the FeedForward Block<a class="anchor" aria-hidden="true" href="#implemention-of-the-feedforward-block" hidden="">#</a>
</h3>
<pre data-lang="python" style="background-color:#383838;color:#e6e1dc;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#f92672dd;">import </span><span>torch.nn </span><span style="color:#cc7833;">as </span><span>nn
</span><span>
</span><span style="font-style:italic;color:#cc7833;">class </span><span style="text-decoration:underline;color:#ffc66d;">FeedForwardBlock</span><span>(</span><span style="text-decoration:underline;font-style:italic;color:#ffc66d;">nn.Module</span><span>):
</span><span>    </span><span style="color:#95815e;">&quot;&quot;&quot;
</span><span style="color:#95815e;">    A feed-forward block in the Transformer model.
</span><span style="color:#95815e;">
</span><span style="color:#95815e;">    Args:
</span><span style="color:#95815e;">        embedding_dim (int): The dimensionality of the input embeddings.
</span><span style="color:#95815e;">        feed_forward_dim (int): The dimensionality of the hidden layer in the feed-forward network.
</span><span style="color:#95815e;">        dropout (float): The dropout probability.
</span><span style="color:#95815e;">
</span><span style="color:#95815e;">    Attributes:
</span><span style="color:#95815e;">        linear_1 (nn.Linear): The first linear layer.
</span><span style="color:#95815e;">        dropout (nn.Dropout): The dropout layer.
</span><span style="color:#95815e;">        linear_2 (nn.Linear): The second linear layer.
</span><span style="color:#95815e;">    &quot;&quot;&quot;
</span><span>
</span><span>    </span><span style="font-style:italic;color:#cc7833;">def </span><span style="color:#da4939;">__init__</span><span>(</span><span style="font-style:italic;color:#fd971f;">self</span><span>, </span><span style="font-style:italic;color:#fd971f;">embedding_dim</span><span>, </span><span style="font-style:italic;color:#fd971f;">feed_forward_dim</span><span>, </span><span style="font-style:italic;color:#fd971f;">dropout</span><span>) -&gt; </span><span style="color:#6e9cbe;">None</span><span>:
</span><span>        </span><span style="color:#da4939;">super</span><span>().</span><span style="color:#da4939;">__init__</span><span>()
</span><span>        </span><span style="color:#d0d0ff;">self</span><span>.linear_1 </span><span style="color:#cc7833;">= </span><span>nn.Linear(embedding_dim, feed_forward_dim)
</span><span>        </span><span style="color:#d0d0ff;">self</span><span>.dropout </span><span style="color:#cc7833;">= </span><span>nn.Dropout(dropout)
</span><span>        </span><span style="color:#d0d0ff;">self</span><span>.linear_2 </span><span style="color:#cc7833;">= </span><span>nn.Linear(feed_forward_dim, embedding_dim)
</span><span>        
</span><span>    </span><span style="font-style:italic;color:#cc7833;">def </span><span style="color:#ffc66d;">forward</span><span>(</span><span style="font-style:italic;color:#fd971f;">self</span><span>, </span><span style="font-style:italic;color:#fd971f;">x</span><span>):
</span><span>        </span><span style="color:#95815e;">&quot;&quot;&quot;
</span><span style="color:#95815e;">        Forward pass of the feed-forward block.
</span><span style="color:#95815e;">
</span><span style="color:#95815e;">        Args:
</span><span style="color:#95815e;">            x (torch.Tensor): The input tensor.
</span><span style="color:#95815e;">
</span><span style="color:#95815e;">        Returns:
</span><span style="color:#95815e;">            torch.Tensor: The output tensor.
</span><span style="color:#95815e;">        &quot;&quot;&quot;
</span><span>        x </span><span style="color:#cc7833;">= </span><span style="color:#d0d0ff;">self</span><span>.linear_1(x)
</span><span>        x </span><span style="color:#cc7833;">= </span><span>nn.ReLU(x)
</span><span>        x </span><span style="color:#cc7833;">= </span><span style="color:#d0d0ff;">self</span><span>.dropout(x)
</span><span>        x </span><span style="color:#cc7833;">= </span><span style="color:#d0d0ff;">self</span><span>.linear_2(x)
</span><span>        </span><span style="color:#cc7833;">return </span><span>x 
</span></code></pre>
<h3 id="multihead-attention-block">Multihead Attention Block<a class="anchor" aria-hidden="true" href="#multihead-attention-block" hidden="">#</a>
</h3>
<p>The Multi-Head Attention block receives the input data split into queries, keys, and values organized into matrices ùëÑ, ùêæ, and ùëâ. Each matrix contains different facets of the input, and they have the same dimensions as the input.We then linearly transform each matrix by their respective weight matrices ùëä^Q, ùëä^K, and ùëä^V. These transformations will result in new matrices ùëÑ‚Ä≤, ùêæ‚Ä≤, and ùëâ‚Ä≤, which will be split into smaller matrices corresponding to different heads ‚Ñé, allowing the model to attend to information from different representation subspaces in parallel. This split creates multiple sets of queries, keys, and values for each head. Finally, we concatenate every head into an ùêª matrix, which is then transformed by another weight matrix ùëäùëú to produce the multi-head attention output, a matrix ùëÄùêª‚àíùê¥ that retains the input dimensionality.</p>
<h3 id="implementation-of-multihead-attention-block">Implementation of Multihead Attention Block<a class="anchor" aria-hidden="true" href="#implementation-of-multihead-attention-block" hidden="">#</a>
</h3>
<pre data-lang="python" style="background-color:#383838;color:#e6e1dc;" class="language-python "><code class="language-python" data-lang="python"><span>
</span><span>
</span><span style="color:#f92672dd;">import </span><span>torch
</span><span style="color:#f92672dd;">import </span><span>torch.nn </span><span style="color:#cc7833;">as </span><span>nn
</span><span>
</span><span>
</span><span style="font-style:italic;color:#cc7833;">class </span><span style="text-decoration:underline;color:#ffc66d;">MultiHeadAttention</span><span>(</span><span style="text-decoration:underline;font-style:italic;color:#ffc66d;">nn.Module</span><span>):
</span><span>    </span><span style="font-style:italic;color:#cc7833;">def </span><span style="color:#da4939;">__init__</span><span>(</span><span style="font-style:italic;color:#fd971f;">self</span><span>, </span><span style="font-style:italic;color:#fd971f;">embedding_dim</span><span>, </span><span style="font-style:italic;color:#fd971f;">num_heads</span><span>):
</span><span>        </span><span style="color:#95815e;">&quot;&quot;&quot;
</span><span style="color:#95815e;">        Initializes the MultiHeadAttention module.
</span><span style="color:#95815e;">
</span><span style="color:#95815e;">        Args:
</span><span style="color:#95815e;">            embedding_dim (int): The input and output dimension of the model.
</span><span style="color:#95815e;">            num_heads (int): The number of attention heads.
</span><span style="color:#95815e;">
</span><span style="color:#95815e;">        Raises:
</span><span style="color:#95815e;">            AssertionError: If embedding_dim is not divisible by num_heads.
</span><span style="color:#95815e;">        &quot;&quot;&quot;
</span><span>        </span><span style="color:#da4939;">super</span><span>(MultiHeadAttention, </span><span style="color:#d0d0ff;">self</span><span>).</span><span style="color:#da4939;">__init__</span><span>()
</span><span>        </span><span style="color:#cc7833;">assert </span><span>embedding_dim </span><span style="color:#cc7833;">% </span><span>num_heads </span><span style="color:#cc7833;">== </span><span style="color:#a5c261;">0</span><span>, </span><span style="color:#c1be91;">&quot;embedding_dim must be divisible by num_heads&quot;
</span><span>        
</span><span>        </span><span style="color:#d0d0ff;">self</span><span>.embedding_dim </span><span style="color:#cc7833;">= </span><span>embedding_dim
</span><span>        </span><span style="color:#d0d0ff;">self</span><span>.num_heads </span><span style="color:#cc7833;">= </span><span>num_heads
</span><span>        </span><span style="color:#d0d0ff;">self</span><span>.d_k </span><span style="color:#cc7833;">= </span><span style="color:#d0d0ff;">self</span><span>.embedding_dim </span><span style="color:#cc7833;">// </span><span>num_heads
</span><span>        
</span><span>        </span><span style="color:#d0d0ff;">self</span><span>.W_q </span><span style="color:#cc7833;">= </span><span>nn.Linear(embedding_dim,embedding_dim)
</span><span>        </span><span style="color:#d0d0ff;">self</span><span>.W_k </span><span style="color:#cc7833;">= </span><span>nn.Linear(embedding_dim, embedding_dim)
</span><span>        </span><span style="color:#d0d0ff;">self</span><span>.W_v </span><span style="color:#cc7833;">= </span><span>nn.Linear(embedding_dim, embedding_dim)
</span><span>        </span><span style="color:#d0d0ff;">self</span><span>.W_o </span><span style="color:#cc7833;">= </span><span>nn.Linear(embedding_dim, embedding_dim)
</span><span>        
</span><span>    </span><span style="font-style:italic;color:#cc7833;">def </span><span style="color:#ffc66d;">scaled_dot_product_attention</span><span>(</span><span style="font-style:italic;color:#fd971f;">self</span><span>, </span><span style="font-style:italic;color:#fd971f;">Q</span><span>, </span><span style="font-style:italic;color:#fd971f;">K</span><span>, </span><span style="font-style:italic;color:#fd971f;">V</span><span>, </span><span style="font-style:italic;color:#fd971f;">mask</span><span style="color:#cc7833;">=</span><span style="color:#6e9cbe;">None</span><span>):
</span><span>        </span><span style="color:#95815e;">&quot;&quot;&quot;
</span><span style="color:#95815e;">        Performs scaled dot product attention.
</span><span style="color:#95815e;">
</span><span style="color:#95815e;">        Args:
</span><span style="color:#95815e;">            Q (torch.Tensor): The query tensor of shape (batch_size, seq_length, embedding_dim).
</span><span style="color:#95815e;">            K (torch.Tensor): The key tensor of shape (batch_size, seq_length, embedding_dim).
</span><span style="color:#95815e;">            V (torch.Tensor): The value tensor of shape (batch_size, seq_length, embedding_dim).
</span><span style="color:#95815e;">            mask (torch.Tensor, optional): The attention mask tensor of shape (batch_size, seq_length, seq_length).
</span><span style="color:#95815e;">
</span><span style="color:#95815e;">        Returns:
</span><span style="color:#95815e;">            torch.Tensor: The output tensor of shape (batch_size, seq_length, embedding_dim).
</span><span style="color:#95815e;">        &quot;&quot;&quot;
</span><span>        attn_scores </span><span style="color:#cc7833;">= </span><span>torch.matmul(Q, K.transpose(</span><span style="color:#cc7833;">-</span><span style="color:#a5c261;">2</span><span>, </span><span style="color:#cc7833;">-</span><span style="color:#a5c261;">1</span><span>)) </span><span style="color:#cc7833;">/ </span><span>math.sqrt(</span><span style="color:#d0d0ff;">self</span><span>.d_k)
</span><span>        </span><span style="color:#cc7833;">if </span><span>mask </span><span style="color:#cc7833;">is not </span><span style="color:#6e9cbe;">None</span><span>:
</span><span>            attn_scores </span><span style="color:#cc7833;">= </span><span>attn_scores.masked_fill(mask </span><span style="color:#cc7833;">== </span><span style="color:#a5c261;">0</span><span>, </span><span style="color:#cc7833;">-</span><span style="color:#a5c261;">1e9</span><span>)
</span><span>        attn_probs </span><span style="color:#cc7833;">= </span><span>torch.softmax(attn_scores, </span><span style="font-style:italic;color:#fd971f;">dim</span><span style="color:#cc7833;">=-</span><span style="color:#a5c261;">1</span><span>)
</span><span>        output </span><span style="color:#cc7833;">= </span><span>torch.matmul(attn_probs, V)
</span><span>        </span><span style="color:#cc7833;">return </span><span>output
</span><span>        
</span><span>    </span><span style="font-style:italic;color:#cc7833;">def </span><span style="color:#ffc66d;">split_heads</span><span>(</span><span style="font-style:italic;color:#fd971f;">self</span><span>, </span><span style="font-style:italic;color:#fd971f;">x</span><span>):
</span><span>        </span><span style="color:#95815e;">&quot;&quot;&quot;
</span><span style="color:#95815e;">        Splits the input tensor into multiple heads.
</span><span style="color:#95815e;">
</span><span style="color:#95815e;">        Args:
</span><span style="color:#95815e;">            x (torch.Tensor): The input tensor of shape (batch_size, seq_length, embedding_dim).
</span><span style="color:#95815e;">
</span><span style="color:#95815e;">        Returns:
</span><span style="color:#95815e;">            torch.Tensor: The tensor with shape (batch_size, num_heads, seq_length, d_k).
</span><span style="color:#95815e;">        &quot;&quot;&quot;
</span><span>        batch_size, seq_length, embedding_dim </span><span style="color:#cc7833;">= </span><span>x.size()
</span><span>        </span><span style="color:#cc7833;">return </span><span>x.view(batch_size, seq_length, </span><span style="color:#d0d0ff;">self</span><span>.num_heads, </span><span style="color:#d0d0ff;">self</span><span>.d_k).transpose(</span><span style="color:#a5c261;">1</span><span>, </span><span style="color:#a5c261;">2</span><span>)
</span><span>        
</span><span>    </span><span style="font-style:italic;color:#cc7833;">def </span><span style="color:#ffc66d;">combine_heads</span><span>(</span><span style="font-style:italic;color:#fd971f;">self</span><span>, </span><span style="font-style:italic;color:#fd971f;">x</span><span>):
</span><span>        </span><span style="color:#95815e;">&quot;&quot;&quot;
</span><span style="color:#95815e;">        Combines the heads of the input tensor.
</span><span style="color:#95815e;">
</span><span style="color:#95815e;">        Args:
</span><span style="color:#95815e;">            x (torch.Tensor): The input tensor of shape (batch_size, num_heads, seq_length, d_k).
</span><span style="color:#95815e;">
</span><span style="color:#95815e;">        Returns:
</span><span style="color:#95815e;">            torch.Tensor: The tensor with shape (batch_size, seq_length, embedding_dim).
</span><span style="color:#95815e;">        &quot;&quot;&quot;
</span><span>        batch_size, </span><span style="color:#d0d0ff;">_</span><span>, seq_length, d_k </span><span style="color:#cc7833;">= </span><span>x.size()
</span><span>        </span><span style="color:#cc7833;">return </span><span>x.transpose(</span><span style="color:#a5c261;">1</span><span>, </span><span style="color:#a5c261;">2</span><span>).contiguous().view(batch_size, seq_length, </span><span style="color:#d0d0ff;">self</span><span>.embedding_dim)
</span><span>        
</span><span>    </span><span style="font-style:italic;color:#cc7833;">def </span><span style="color:#ffc66d;">forward</span><span>(</span><span style="font-style:italic;color:#fd971f;">self</span><span>, </span><span style="font-style:italic;color:#fd971f;">Q</span><span>, </span><span style="font-style:italic;color:#fd971f;">K</span><span>, </span><span style="font-style:italic;color:#fd971f;">V</span><span>, </span><span style="font-style:italic;color:#fd971f;">mask</span><span style="color:#cc7833;">=</span><span style="color:#6e9cbe;">None</span><span>):
</span><span>        </span><span style="color:#95815e;">&quot;&quot;&quot;
</span><span style="color:#95815e;">        Performs forward pass of the MultiHeadAttention module.
</span><span style="color:#95815e;">
</span><span style="color:#95815e;">        Args:
</span><span style="color:#95815e;">            Q (torch.Tensor): The query tensor of shape (batch_size, seq_length, embedding_dim).
</span><span style="color:#95815e;">            K (torch.Tensor): The key tensor of shape (batch_size, seq_length, embedding_dim).
</span><span style="color:#95815e;">            V (torch.Tensor): The value tensor of shape (batch_size, seq_length, embedding_dim).
</span><span style="color:#95815e;">            mask (torch.Tensor, optional): The attention mask tensor of shape (batch_size, seq_length, seq_length).
</span><span style="color:#95815e;">
</span><span style="color:#95815e;">        Returns:
</span><span style="color:#95815e;">            torch.Tensor: The output tensor of shape (batch_size, seq_length, embedding_dim).
</span><span style="color:#95815e;">        &quot;&quot;&quot;
</span><span>        Q </span><span style="color:#cc7833;">= </span><span style="color:#d0d0ff;">self</span><span>.split_heads(</span><span style="color:#d0d0ff;">self</span><span>.W_q(Q))
</span><span>        K </span><span style="color:#cc7833;">= </span><span style="color:#d0d0ff;">self</span><span>.split_heads(</span><span style="color:#d0d0ff;">self</span><span>.W_k(K))
</span><span>        V </span><span style="color:#cc7833;">= </span><span style="color:#d0d0ff;">self</span><span>.split_heads(</span><span style="color:#d0d0ff;">self</span><span>.W_v(V))
</span><span>        
</span><span>        attn_output </span><span style="color:#cc7833;">= </span><span style="color:#d0d0ff;">self</span><span>.scaled_dot_product_attention(Q, K, V, mask)
</span><span>        output </span><span style="color:#cc7833;">= </span><span style="color:#d0d0ff;">self</span><span>.W_o(</span><span style="color:#d0d0ff;">self</span><span>.combine_heads(attn_output))
</span><span>        </span><span style="color:#cc7833;">return </span><span>output
</span><span>
</span></code></pre>
<h3 id="encoder-block">Encoder Block<a class="anchor" aria-hidden="true" href="#encoder-block" hidden="">#</a>
</h3>
<p>An Encoder layer consists of a Multi-Head Attention layer, a Position-wise Feed-Forward layer, and two Layer Normalization layers.
The EncoderLayer class initializes with input parameters and components, including a MultiHeadAttention module, a PositionWiseFeedForward module, two layer normalization modules, and a dropout layer. The forward methods computes the encoder layer output by applying self-attention, adding the attention output to the input tensor, and normalizing the result. Then, it computes the position-wise feed-forward output, combines it with the normalized self-attention output, and normalizes the final result before returning the processed tensor.</p>
<h3 id="implementation-of-the-encoder-block">Implementation of the Encoder Block<a class="anchor" aria-hidden="true" href="#implementation-of-the-encoder-block" hidden="">#</a>
</h3>
<pre data-lang="python" style="background-color:#383838;color:#e6e1dc;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#f92672dd;">import </span><span>torch
</span><span style="color:#f92672dd;">import </span><span>torch.nn </span><span style="color:#cc7833;">as </span><span>nn 
</span><span>
</span><span>
</span><span style="font-style:italic;color:#cc7833;">class </span><span style="text-decoration:underline;color:#ffc66d;">EncoderBlock</span><span>(</span><span style="text-decoration:underline;font-style:italic;color:#ffc66d;">nn.Module</span><span>):
</span><span>    </span><span style="font-style:italic;color:#cc7833;">def </span><span style="color:#da4939;">__init__</span><span>(</span><span style="font-style:italic;color:#fd971f;">self</span><span>, </span><span style="font-style:italic;color:#fd971f;">embedding_dim</span><span>, </span><span style="font-style:italic;color:#fd971f;">num_heads</span><span>, </span><span style="font-style:italic;color:#fd971f;">feed_forward_dim</span><span>, </span><span style="font-style:italic;color:#fd971f;">dropout</span><span>):
</span><span>        </span><span style="color:#95815e;">&quot;&quot;&quot;
</span><span style="color:#95815e;">        Initializes an EncoderLayer module.
</span><span style="color:#95815e;">
</span><span style="color:#95815e;">        Args:
</span><span style="color:#95815e;">            embedding_dim (int): The dimensionality of the input and output feature vectors.
</span><span style="color:#95815e;">            num_heads (int): The number of attention heads.
</span><span style="color:#95815e;">            feed_forward_dim (int): The dimensionality of the feed-forward layer.
</span><span style="color:#95815e;">            dropout (float): The dropout probability.
</span><span style="color:#95815e;">
</span><span style="color:#95815e;">        &quot;&quot;&quot;
</span><span>        </span><span style="color:#da4939;">super</span><span>(EncoderLayer, </span><span style="color:#d0d0ff;">self</span><span>).</span><span style="color:#da4939;">__init__</span><span>()
</span><span>        </span><span style="color:#d0d0ff;">self</span><span>.self_attn </span><span style="color:#cc7833;">= </span><span>MultiHeadAttention(embedding_dim, num_heads)
</span><span>        </span><span style="color:#d0d0ff;">self</span><span>.feed_forward </span><span style="color:#cc7833;">= </span><span>FeedForward(embedding_dim,feed_forward_dim)
</span><span>        </span><span style="color:#d0d0ff;">self</span><span>.norm1 </span><span style="color:#cc7833;">= </span><span>LayerNormalization(embedding_dim)
</span><span>        </span><span style="color:#d0d0ff;">self</span><span>.norm2 </span><span style="color:#cc7833;">= </span><span>LayerNormalization(embedding_dim)
</span><span>        </span><span style="color:#d0d0ff;">self</span><span>.dropout </span><span style="color:#cc7833;">= </span><span>nn.Dropout(dropout)
</span><span>        
</span><span>    </span><span style="font-style:italic;color:#cc7833;">def </span><span style="color:#ffc66d;">forward</span><span>(</span><span style="font-style:italic;color:#fd971f;">self</span><span>, </span><span style="font-style:italic;color:#fd971f;">x</span><span>, </span><span style="font-style:italic;color:#fd971f;">mask</span><span>):
</span><span>        </span><span style="color:#95815e;">&quot;&quot;&quot;
</span><span style="color:#95815e;">        Performs a forward pass of the EncoderLayer module.
</span><span style="color:#95815e;">
</span><span style="color:#95815e;">        Args:
</span><span style="color:#95815e;">            x (torch.Tensor): The input tensor of shape (batch_size, seq_len, d_model).
</span><span style="color:#95815e;">            mask (torch.Tensor): The attention mask tensor of shape (batch_size, seq_len, seq_len).
</span><span style="color:#95815e;">
</span><span style="color:#95815e;">        Returns:
</span><span style="color:#95815e;">            torch.Tensor: The output tensor of shape (batch_size, seq_len, d_model).
</span><span style="color:#95815e;">
</span><span style="color:#95815e;">        &quot;&quot;&quot;
</span><span>        attn_output </span><span style="color:#cc7833;">= </span><span style="color:#d0d0ff;">self</span><span>.self_attn(x, x, x, mask)
</span><span>        x </span><span style="color:#cc7833;">= </span><span style="color:#d0d0ff;">self</span><span>.norm1(x </span><span style="color:#cc7833;">+ </span><span style="color:#d0d0ff;">self</span><span>.dropout(attn_output))
</span><span>        ff_output </span><span style="color:#cc7833;">= </span><span style="color:#d0d0ff;">self</span><span>.feed_forward(x)
</span><span>        x </span><span style="color:#cc7833;">= </span><span style="color:#d0d0ff;">self</span><span>.norm2(x </span><span style="color:#cc7833;">+ </span><span style="color:#d0d0ff;">self</span><span>.dropout(ff_output))
</span><span>        </span><span style="color:#cc7833;">return </span><span>x
</span></code></pre>
<h3 id="decoder-block">Decoder Block<a class="anchor" aria-hidden="true" href="#decoder-block" hidden="">#</a>
</h3>
<p>After Encoder the Keys and Values from the output are consumed and Query is provided by the Output Embedding in the Decoder the decoder layer consists of two Multi-Head Attention layers, a Position-wise Feed-Forward layer, and three Layer Normalization layers. The forward method computes the decoder layer output by performing the following steps:</p>
<ul>
<li>Calculate the masked self-attention output and add it to the input tensor, followed by dropout and layer normalization.</li>
<li>Compute the cross-attention output between the decoder and encoder outputs, and add it to the normalized masked self-attention output, followed by dropout and layer normalization.</li>
<li>Calculate the position-wise feed-forward output and combine it with the normalized cross-attention output, followed by dropout and layer normalization.</li>
<li>Return the processed tensor.</li>
<li>These operations enable the decoder to generate target sequences based on the input and the encoder output.</li>
</ul>
<h3 id="implementation-of-the-decoder-block">Implementation of the Decoder Block<a class="anchor" aria-hidden="true" href="#implementation-of-the-decoder-block" hidden="">#</a>
</h3>
<pre data-lang="python" style="background-color:#383838;color:#e6e1dc;" class="language-python "><code class="language-python" data-lang="python"><span>
</span><span style="color:#f92672dd;">import </span><span>torch 
</span><span style="color:#f92672dd;">import </span><span>torch.nn </span><span style="color:#cc7833;">as </span><span>nn
</span><span>
</span><span style="font-style:italic;color:#cc7833;">class </span><span style="text-decoration:underline;color:#ffc66d;">DecoderBlock</span><span>(</span><span style="text-decoration:underline;font-style:italic;color:#ffc66d;">nn.Module</span><span>):
</span><span>    </span><span style="font-style:italic;color:#cc7833;">def </span><span style="color:#da4939;">__init__</span><span>(</span><span style="font-style:italic;color:#fd971f;">self</span><span>, </span><span style="font-style:italic;color:#fd971f;">embedding_dim</span><span>, </span><span style="font-style:italic;color:#fd971f;">num_heads</span><span>, </span><span style="font-style:italic;color:#fd971f;">feed_forward_dim</span><span>, </span><span style="font-style:italic;color:#fd971f;">dropout</span><span>):
</span><span>        </span><span style="color:#95815e;">&quot;&quot;&quot;
</span><span style="color:#95815e;">        Initializes a DecoderLayer module.
</span><span style="color:#95815e;">
</span><span style="color:#95815e;">        Args:
</span><span style="color:#95815e;">            embedding_dim (int): The dimension of the input embeddings.
</span><span style="color:#95815e;">            num_heads (int): The number of attention heads.
</span><span style="color:#95815e;">            feed_forward_dim (int): The dimension of the feed-forward layer.
</span><span style="color:#95815e;">            dropout (float): The dropout probability.
</span><span style="color:#95815e;">
</span><span style="color:#95815e;">        &quot;&quot;&quot;
</span><span>        </span><span style="color:#da4939;">super</span><span>(DecoderLayer, </span><span style="color:#d0d0ff;">self</span><span>).</span><span style="color:#da4939;">__init__</span><span>()
</span><span>        </span><span style="color:#d0d0ff;">self</span><span>.self_attn </span><span style="color:#cc7833;">= </span><span>MultiHeadAttention(embedding_dim, num_heads)
</span><span>        </span><span style="color:#d0d0ff;">self</span><span>.cross_attn </span><span style="color:#cc7833;">= </span><span>MultiHeadAttention(embedding_dim, num_heads)
</span><span>        </span><span style="color:#d0d0ff;">self</span><span>.feed_forward </span><span style="color:#cc7833;">= </span><span>FeedForwardBlock(embedding_dim, feed_forward_dim)
</span><span>        </span><span style="color:#d0d0ff;">self</span><span>.norm1 </span><span style="color:#cc7833;">= </span><span>LayerNormalization(d_model)
</span><span>        </span><span style="color:#d0d0ff;">self</span><span>.norm2 </span><span style="color:#cc7833;">= </span><span>LayerNormalization(d_model)
</span><span>        </span><span style="color:#d0d0ff;">self</span><span>.norm3 </span><span style="color:#cc7833;">= </span><span>LayerNormalization(d_model)
</span><span>        </span><span style="color:#d0d0ff;">self</span><span>.dropout </span><span style="color:#cc7833;">= </span><span>nn.Dropout(dropout)
</span><span>        
</span><span>    </span><span style="font-style:italic;color:#cc7833;">def </span><span style="color:#ffc66d;">forward</span><span>(</span><span style="font-style:italic;color:#fd971f;">self</span><span>, </span><span style="font-style:italic;color:#fd971f;">x</span><span>, </span><span style="font-style:italic;color:#fd971f;">enc_output</span><span>, </span><span style="font-style:italic;color:#fd971f;">src_mask</span><span>, </span><span style="font-style:italic;color:#fd971f;">tgt_mask</span><span>):
</span><span>        </span><span style="color:#95815e;">&quot;&quot;&quot;
</span><span style="color:#95815e;">        Performs a forward pass of the DecoderLayer module.
</span><span style="color:#95815e;">
</span><span style="color:#95815e;">        Args:
</span><span style="color:#95815e;">            x (torch.Tensor): The input tensor.
</span><span style="color:#95815e;">            enc_output (torch.Tensor): The output of the encoder.
</span><span style="color:#95815e;">            src_mask (torch.Tensor): The mask for the source sequence.
</span><span style="color:#95815e;">            tgt_mask (torch.Tensor): The mask for the target sequence.
</span><span style="color:#95815e;">
</span><span style="color:#95815e;">        Returns:
</span><span style="color:#95815e;">            torch.Tensor: The output tensor.
</span><span style="color:#95815e;">
</span><span style="color:#95815e;">        &quot;&quot;&quot;
</span><span>        attn_output </span><span style="color:#cc7833;">= </span><span style="color:#d0d0ff;">self</span><span>.self_attn(x, x, x, tgt_mask)
</span><span>        x </span><span style="color:#cc7833;">= </span><span style="color:#d0d0ff;">self</span><span>.norm1(x </span><span style="color:#cc7833;">+ </span><span style="color:#d0d0ff;">self</span><span>.dropout(attn_output))
</span><span>        attn_output </span><span style="color:#cc7833;">= </span><span style="color:#d0d0ff;">self</span><span>.cross_attn(x, enc_output, enc_output, src_mask)
</span><span>        x </span><span style="color:#cc7833;">= </span><span style="color:#d0d0ff;">self</span><span>.norm2(x </span><span style="color:#cc7833;">+ </span><span style="color:#d0d0ff;">self</span><span>.dropout(attn_output))
</span><span>        ff_output </span><span style="color:#cc7833;">= </span><span style="color:#d0d0ff;">self</span><span>.feed_forward(x)
</span><span>        x </span><span style="color:#cc7833;">= </span><span style="color:#d0d0ff;">self</span><span>.norm3(x </span><span style="color:#cc7833;">+ </span><span style="color:#d0d0ff;">self</span><span>.dropout(ff_output))
</span><span>        </span><span style="color:#cc7833;">return </span><span>x
</span></code></pre>
<h3 id="building-the-transformer">Building the Transformer<a class="anchor" aria-hidden="true" href="#building-the-transformer" hidden="">#</a>
</h3>
<p>Now that we have built every layer Lets combine them to Build the Transformer</p>
<pre data-lang="python" style="background-color:#383838;color:#e6e1dc;" class="language-python "><code class="language-python" data-lang="python"><span>
</span><span style="font-style:italic;color:#cc7833;">class </span><span style="text-decoration:underline;color:#ffc66d;">Transformer</span><span>(</span><span style="text-decoration:underline;font-style:italic;color:#ffc66d;">nn.Module</span><span>):
</span><span>    </span><span style="font-style:italic;color:#cc7833;">def </span><span style="color:#da4939;">__init__</span><span>(</span><span style="font-style:italic;color:#fd971f;">self</span><span>, </span><span style="font-style:italic;color:#fd971f;">src_vocab_size</span><span>, </span><span style="font-style:italic;color:#fd971f;">tgt_vocab_size</span><span>, </span><span style="font-style:italic;color:#fd971f;">embedding_dim</span><span>, </span><span style="font-style:italic;color:#fd971f;">num_heads</span><span>, </span><span style="font-style:italic;color:#fd971f;">num_layers</span><span>, </span><span style="font-style:italic;color:#fd971f;">feed_forward_dim</span><span>, </span><span style="font-style:italic;color:#fd971f;">max_seq_length</span><span>, </span><span style="font-style:italic;color:#fd971f;">dropout</span><span>):
</span><span>        </span><span style="color:#95815e;">&quot;&quot;&quot;
</span><span style="color:#95815e;">        Initializes the Transformer model.
</span><span style="color:#95815e;">
</span><span style="color:#95815e;">        Args:
</span><span style="color:#95815e;">        - src_vocab_size (int): The size of the source vocabulary.
</span><span style="color:#95815e;">        - tgt_vocab_size (int): The size of the target vocabulary.
</span><span style="color:#95815e;">        - embedding_dim (int): The dimension of the word embeddings.
</span><span style="color:#95815e;">        - num_heads (int): The number of attention heads.
</span><span style="color:#95815e;">        - num_layers (int): The number of encoder and decoder layers.
</span><span style="color:#95815e;">        - feed_forward_dim (int): The dimension of the feed-forward layer.
</span><span style="color:#95815e;">        - max_seq_length (int): The maximum sequence length.
</span><span style="color:#95815e;">        - dropout (float): The dropout rate.
</span><span style="color:#95815e;">
</span><span style="color:#95815e;">        &quot;&quot;&quot;
</span><span>        </span><span style="color:#da4939;">super</span><span>(Transformer, </span><span style="color:#d0d0ff;">self</span><span>).</span><span style="color:#da4939;">__init__</span><span>()
</span><span>        </span><span style="color:#d0d0ff;">self</span><span>.encoder_embedding </span><span style="color:#cc7833;">= </span><span>InputEmbeddingBlock(src_vocab_size, embedding_dim)
</span><span>        </span><span style="color:#d0d0ff;">self</span><span>.decoder_embedding </span><span style="color:#cc7833;">= </span><span>InputEmbeddingBlock(tgt_vocab_size, embedding_dim)
</span><span>        </span><span style="color:#d0d0ff;">self</span><span>.positional_encoding </span><span style="color:#cc7833;">= </span><span>PositionalEncoding(embedding_dim, max_seq_length)
</span><span>        </span><span style="color:#d0d0ff;">self</span><span>.encoder_layers </span><span style="color:#cc7833;">= </span><span>nn.ModuleList([EncoderBlock(embedding_dim, num_heads, feed_forward_dim, dropout) </span><span style="color:#cc7833;">for </span><span style="color:#d0d0ff;">_ </span><span style="color:#cc7833;">in </span><span style="color:#da4939;">range</span><span>(num_layers)])
</span><span>        </span><span style="color:#d0d0ff;">self</span><span>.decoder_layers </span><span style="color:#cc7833;">= </span><span>nn.ModuleList([DecoderBlock(embedding_dim, num_heads, feed_forward_dim, dropout) </span><span style="color:#cc7833;">for </span><span style="color:#d0d0ff;">_ </span><span style="color:#cc7833;">in </span><span style="color:#da4939;">range</span><span>(num_layers)])
</span><span>        </span><span style="color:#d0d0ff;">self</span><span>.fc </span><span style="color:#cc7833;">= </span><span>nn.Linear(embedding_dim, tgt_vocab_size)
</span><span>        </span><span style="color:#d0d0ff;">self</span><span>.dropout </span><span style="color:#cc7833;">= </span><span>nn.Dropout(dropout)
</span><span>
</span><span>    </span><span style="font-style:italic;color:#cc7833;">def </span><span style="color:#ffc66d;">generate_mask</span><span>(</span><span style="font-style:italic;color:#fd971f;">self</span><span>, </span><span style="font-style:italic;color:#fd971f;">src</span><span>, </span><span style="font-style:italic;color:#fd971f;">tgt</span><span>):
</span><span>        </span><span style="color:#95815e;">&quot;&quot;&quot;
</span><span style="color:#95815e;">        Generates masks for the source and target sequences.
</span><span style="color:#95815e;">
</span><span style="color:#95815e;">        Args:
</span><span style="color:#95815e;">        - src (Tensor): The source sequence tensor.
</span><span style="color:#95815e;">        - tgt (Tensor): The target sequence tensor.
</span><span style="color:#95815e;">
</span><span style="color:#95815e;">        Returns:
</span><span style="color:#95815e;">        - src_mask (Tensor): The source mask tensor.
</span><span style="color:#95815e;">        - tgt_mask (Tensor): The target mask tensor.
</span><span style="color:#95815e;">
</span><span style="color:#95815e;">        &quot;&quot;&quot;
</span><span>        src_mask </span><span style="color:#cc7833;">= </span><span>(src </span><span style="color:#cc7833;">!= </span><span style="color:#a5c261;">0</span><span>).unsqueeze(</span><span style="color:#a5c261;">1</span><span>).unsqueeze(</span><span style="color:#a5c261;">2</span><span>)
</span><span>        tgt_mask </span><span style="color:#cc7833;">= </span><span>(tgt </span><span style="color:#cc7833;">!= </span><span style="color:#a5c261;">0</span><span>).unsqueeze(</span><span style="color:#a5c261;">1</span><span>).unsqueeze(</span><span style="color:#a5c261;">3</span><span>)
</span><span>        seq_length </span><span style="color:#cc7833;">= </span><span>tgt.size(</span><span style="color:#a5c261;">1</span><span>)
</span><span>        nopeak_mask </span><span style="color:#cc7833;">= </span><span>(</span><span style="color:#a5c261;">1 </span><span style="color:#cc7833;">- </span><span>torch.triu(torch.ones(</span><span style="color:#a5c261;">1</span><span>, seq_length, seq_length), </span><span style="font-style:italic;color:#fd971f;">diagonal</span><span style="color:#cc7833;">=</span><span style="color:#a5c261;">1</span><span>)).bool()
</span><span>        tgt_mask </span><span style="color:#cc7833;">= </span><span>tgt_mask </span><span style="color:#cc7833;">&amp; </span><span>nopeak_mask
</span><span>        </span><span style="color:#cc7833;">return </span><span>src_mask, tgt_mask
</span><span>
</span><span>    </span><span style="font-style:italic;color:#cc7833;">def </span><span style="color:#ffc66d;">forward</span><span>(</span><span style="font-style:italic;color:#fd971f;">self</span><span>, </span><span style="font-style:italic;color:#fd971f;">src</span><span>, </span><span style="font-style:italic;color:#fd971f;">tgt</span><span>):
</span><span>        </span><span style="color:#95815e;">&quot;&quot;&quot;
</span><span style="color:#95815e;">        Performs forward pass of the Transformer model.
</span><span style="color:#95815e;">
</span><span style="color:#95815e;">        Args:
</span><span style="color:#95815e;">        - src (Tensor): The source sequence tensor.
</span><span style="color:#95815e;">        - tgt (Tensor): The target sequence tensor.
</span><span style="color:#95815e;">
</span><span style="color:#95815e;">        Returns:
</span><span style="color:#95815e;">        - output (Tensor): The output tensor.
</span><span style="color:#95815e;">
</span><span style="color:#95815e;">        &quot;&quot;&quot;
</span><span>        src_mask, tgt_mask </span><span style="color:#cc7833;">= </span><span style="color:#d0d0ff;">self</span><span>.generate_mask(src, tgt)
</span><span>        src_embedded </span><span style="color:#cc7833;">= </span><span style="color:#d0d0ff;">self</span><span>.dropout(</span><span style="color:#d0d0ff;">self</span><span>.positional_encoding(</span><span style="color:#d0d0ff;">self</span><span>.encoder_embedding(src)))
</span><span>        tgt_embedded </span><span style="color:#cc7833;">= </span><span style="color:#d0d0ff;">self</span><span>.dropout(</span><span style="color:#d0d0ff;">self</span><span>.positional_encoding(</span><span style="color:#d0d0ff;">self</span><span>.decoder_embedding(tgt)))
</span><span>
</span><span>        enc_output </span><span style="color:#cc7833;">= </span><span>src_embedded
</span><span>        </span><span style="color:#cc7833;">for </span><span>enc_layer </span><span style="color:#cc7833;">in </span><span style="color:#d0d0ff;">self</span><span>.encoder_layers:
</span><span>            enc_output </span><span style="color:#cc7833;">= </span><span>enc_layer(enc_output, src_mask)
</span><span>
</span><span>        dec_output </span><span style="color:#cc7833;">= </span><span>tgt_embedded
</span><span>        </span><span style="color:#cc7833;">for </span><span>dec_layer </span><span style="color:#cc7833;">in </span><span style="color:#d0d0ff;">self</span><span>.decoder_layers:
</span><span>            dec_output </span><span style="color:#cc7833;">= </span><span>dec_layer(dec_output, enc_output, src_mask, tgt_mask)
</span><span>
</span><span>        output </span><span style="color:#cc7833;">= </span><span style="color:#d0d0ff;">self</span><span>.fc(dec_output)
</span><span>        </span><span style="color:#cc7833;">return </span><span>output
</span></code></pre>

  </div>
  

  <footer class="post-footer">
      
      <ul class="post-tags">
          
          
          <li><a href="https:&#x2F;&#x2F;vikramxd.github.io&#x2F;tags&#x2F;ml-research-paper-implementation&#x2F;">ML Research Paper Implementation</a></li>
          
      </ul>
      

    
    


<nav class="paginav">
    
    <a class="prev" href="https:&#x2F;&#x2F;vikramxd.github.io&#x2F;search&#x2F;">
        <span class="title">¬´ Prev</span>
        <br>
        <span>Search</span>
    </a>
    
    
</nav>


    
    
  </footer>

</article>

    </main>
    
    <footer class="footer">
    
    <span>&copy; 2024 <a href="https:&#x2F;&#x2F;vikramxd.github.io"></a></span>
    
    <span>
        Powered by
        <a href="https://www.getzola.org/" rel="noopener noreferrer" target="_blank">Zola</a> &
        <a href="https://github.com/cydave/zola-theme-papermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>


<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>


<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>


<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>



<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>


<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';
        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                var content = codeblock.textContent;
                if(codeblock.firstChild.tagName == 'TABLE') {
                    content = Array(...codeblock.firstChild.getElementsByTagName('span')).map((span) => { return span.textContent; }).join('');
                }
                navigator.clipboard.writeText(content);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            // td containing LineNos
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            // table containing LineNos and code
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            // code blocks not having highlight as parent class
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>



    
</body>
</html>
